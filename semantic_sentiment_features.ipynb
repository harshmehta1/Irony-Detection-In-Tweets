{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\aanch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "f = open('C:/Users/aanch/Desktop/Courses/NLP/Project/SemEval2018-Task3-master/SemEval2018-Task3-master/datasets/train/SemEval2018-T3-train-taskA.txt',encoding=\"utf8\")\n",
    "text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for line in text:\n",
    "    if not line.lower().startswith(\"tweet index\"): # discard first line if it contains metadata\n",
    "        #print(\"line: \",line)\n",
    "        tweet = line.rstrip()\n",
    "        tweets.append(tweet.split('\\t')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_count =[]\n",
    "word_count = []\n",
    "wordmean = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    char_count.append(len(tweet)) #get length of tweet text\n",
    "    words = tweet.split() ##split tweet text into words\n",
    "    word_count.append(len(words))\n",
    "    word_length = []\n",
    "    for i in words:\n",
    "        word_length.append(len(i))\n",
    "    wordmean.append(sum(word_length)/len(word_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "# reduce_len -- Replace repeated character sequences of length 3 or greater with sequences of length 3\n",
    "# strip_handles -- Remove Twitter username handles from text (eg: @xyz)\n",
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True).tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "    token = tokenizer(tweet)\n",
    "    tags = nltk.pos_tag(token)\n",
    "    counts = Counter(tag for word,tag in tags) #Counts only the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun =[]\n",
    "verb = []\n",
    "adverb = []\n",
    "adjective = []\n",
    "for line in tweets:\n",
    "    token= tokenizer(line)\n",
    "    a = nltk.Text(token)\n",
    "    tags=nltk.pos_tag(a)\n",
    "    counts = Counter(tag for word,tag in tags)\n",
    "    count_noun = counts['NN']+counts['NNS']+counts['NNP']+counts['NNPS']\n",
    "    noun.append(count_noun)\n",
    "    count_verb = counts['VB']+counts['VBD']+counts['VBG']+counts['VBN']+counts['VBP']+counts['VBZ']\n",
    "    verb.append(count_verb)\n",
    "    count_adverb = counts['RB']+counts['RBR']+counts['RBS']\n",
    "    adverb.append(count_adverb)\n",
    "    count_adjective = counts['JJ']+counts['JJS']+counts['JJR']\n",
    "    adjective.append(count_adjective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"count_noun: \", count_noun)\n",
    "print(\"count_verb: \", count_verb)\n",
    "print(\"count_adverb: \", count_adverb)\n",
    "print(\"count_adjective: \", count_adjective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aanch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\aanch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "nltk.download('sentiwordnet')\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "tokenizer_regex = RegexpTokenizer(r'\\w+')\n",
    "pattern = '(http.+(\\s)?)|((@\\w*\\d*(\\s)?))' ##Removes url and user handles -- @xyz\n",
    "corp = []\n",
    "for tweet in tweets:\n",
    "    tweet_clean = re.sub(pattern, '', tweet, flags=re.MULTILINE)\n",
    "    corp.append(tweet_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_sum = []\n",
    "negative_sum = []\n",
    "averagesenti = []\n",
    "pos_neg_diff = []\n",
    "\n",
    "for line in corp:\n",
    "    #print(line)\n",
    "    token = tokenizer_regex.tokenize(line)\n",
    "    #print(\"token_regex: \",token)\n",
    "    token = [word for word in token if word not in stopwords.words('english')]\n",
    "    token = [lmtzr.lemmatize(word) for word in token]\n",
    "    #print(\"token: \",token)\n",
    "    pos_scores=[]\n",
    "    neg_scores=[]\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for word in token:\n",
    "        #print(word)\n",
    "        a,b = 0, 0\n",
    "        syn = wordnet.synsets(word)\n",
    "        for sy in syn:\n",
    "            senti = swn.senti_synset(sy.name())\n",
    "            a+=senti.pos_score()\n",
    "            b+=senti.neg_score()\n",
    "        if len(syn) > 0:\n",
    "            a = a/len(syn)\n",
    "            b = b/len(syn)\n",
    "        pos+=a\n",
    "        neg+=b\n",
    "        pos_scores.append(pos)\n",
    "        neg_scores.append(neg)\n",
    "    if(len(pos_scores)!=0):\n",
    "        max_pos =max(pos_scores)\n",
    "        max_neg = max(neg_scores)\n",
    "        pos_sum = sum(pos_scores)\n",
    "        neg_sum = sum(neg_scores)\n",
    "        senti_avg = (pos_sum-neg_sum)/len(token)\n",
    "    else:\n",
    "        max_pos = 0\n",
    "        max_neg =0\n",
    "        pos_sum = 0\n",
    "        neg_sum =0\n",
    "        senti_avg =0\n",
    "\n",
    "    imbal = pos_sum - neg_sum\n",
    "    \n",
    "    positive_sum.append(pos_sum)\n",
    "    negative_sum.append(neg_sum)\n",
    "    averagesenti.append(senti_avg)\n",
    "    pos_neg_diff.append(imbal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"positive_sum: \", positive_sum)\n",
    "print(\"negative_sum: \", negative_sum)\n",
    "print(\"averagesenti: \", averagesenti)\n",
    "print(\"pos_neg_diff: \", pos_neg_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_to_file(data, filename):\n",
    "    fo = open(filename+\".txt\",\"w+\")\n",
    "    for line in data:\n",
    "        fo.write(line)\n",
    "        fo.write(\"\\n\")\n",
    "    fo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
