{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"SemEval2018-T3-train-taskA.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "f = open(file_path,encoding=\"utf8\")\n",
    "text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for line in text:\n",
    "    if not line.lower().startswith(\"tweet index\"): # discard first line if it contains metadata\n",
    "        #print(\"line: \",line)\n",
    "        tweet = line.rstrip()\n",
    "        tweets.append(tweet.split('\\t')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_count =[]\n",
    "word_count = []\n",
    "wordmean = []\n",
    "text_stats =[]\n",
    "for tweet in tweets:\n",
    "    char_count.append(len(tweet)) #get length of tweet text\n",
    "    words = tweet.split() ##split tweet text into words\n",
    "    word_count.append(len(words))\n",
    "    word_length = []\n",
    "    for i in words:\n",
    "        word_length.append(len(i))\n",
    "    wordmean.append(sum(word_length)/len(word_length))\n",
    "    text_stats.append(str(len(tweet))+\"\\t\"+str(len(words))+\"\\t\"+str(sum(word_length)/len(word_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_to_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c337fb16032b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_stats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"text_stats\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"char_count\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\t\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"word_count\"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m\"\\t\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"wordmean_len\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'print_to_file' is not defined"
     ]
    }
   ],
   "source": [
    "print_to_file(text_stats,\"text_stats\",\"char_count\" + \"\\t\" + \"word_count\"+ \"\\t\" + \"wordmean_len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "# reduce_len -- Replace repeated character sequences of length 3 or greater with sequences of length 3\n",
    "# strip_handles -- Remove Twitter username handles from text (eg: @xyz)\n",
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True).tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts = []\n",
    "noun =[]\n",
    "verb = []\n",
    "adverb = []\n",
    "adjective = []\n",
    "for line in tweets:\n",
    "    token= tokenizer(line)\n",
    "    a = nltk.Text(token)\n",
    "    tags=nltk.pos_tag(a)\n",
    "    counts = Counter(tag for word,tag in tags)\n",
    "    count_noun = counts['NN']+counts['NNS']+counts['NNP']+counts['NNPS']\n",
    "    noun.append(count_noun)\n",
    "    count_verb = counts['VB']+counts['VBD']+counts['VBG']+counts['VBN']+counts['VBP']+counts['VBZ']\n",
    "    verb.append(count_verb)\n",
    "    count_adverb = counts['RB']+counts['RBR']+counts['RBS']\n",
    "    adverb.append(count_adverb)\n",
    "    count_adjective = counts['JJ']+counts['JJS']+counts['JJR']\n",
    "    adjective.append(count_adjective)\n",
    "    pos_counts.append(str(count_noun)+\"\\t\"+str(count_verb)+\"\\t\"+str(count_adverb)+\"\\t\"+str(count_adjective))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_to_file(data, filename,column):\n",
    "    fo = open(filename+\".txt\",\"w+\")\n",
    "    \n",
    "    fo.write(column)\n",
    "    fo.write(\"\\n\")\n",
    "    for line in data:\n",
    "        fo.write(str(line))\n",
    "        fo.write(\"\\n\")\n",
    "    fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_to_file(pos_counts,\"POS_tags\",\"count_noun\" + \"\\t\" + \"count_verb\"+ \"\\t\" + \"count_adverb\"+ \"\\t\" + \"count_adjective\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "nltk.download('sentiwordnet')\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "tokenizer_regex = RegexpTokenizer(r'\\w+')\n",
    "pattern = '(http.+(\\s)?)|((@\\w*\\d*(\\s)?))' ##Removes url and user handles -- @xyz\n",
    "corp = []\n",
    "for tweet in tweets:\n",
    "    tweet_clean = re.sub(pattern, '', tweet, flags=re.MULTILINE)\n",
    "    corp.append(tweet_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments =[]\n",
    "positive_sum = []\n",
    "negative_sum = []\n",
    "averagesenti = []\n",
    "pos_neg_diff = []\n",
    "\n",
    "for line in corp:\n",
    "    #print(line)\n",
    "    token = tokenizer_regex.tokenize(line)\n",
    "    #print(\"token_regex: \",token)\n",
    "    token = [word for word in token if word not in stopwords.words('english')]\n",
    "    token = [lmtzr.lemmatize(word) for word in token]\n",
    "    #print(\"token: \",token)\n",
    "    pos_scores=[]\n",
    "    neg_scores=[]\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for word in token:\n",
    "        #print(word)\n",
    "        a,b = 0, 0\n",
    "        syn = wordnet.synsets(word)\n",
    "        for sy in syn:\n",
    "            senti = swn.senti_synset(sy.name())\n",
    "            a+=senti.pos_score()\n",
    "            b+=senti.neg_score()\n",
    "        if len(syn) > 0:\n",
    "            a = a/len(syn)\n",
    "            b = b/len(syn)\n",
    "        pos+=a\n",
    "        neg+=b\n",
    "        pos_scores.append(pos)\n",
    "        neg_scores.append(neg)\n",
    "    if(len(pos_scores)!=0):\n",
    "        max_pos =max(pos_scores)\n",
    "        max_neg = max(neg_scores)\n",
    "        pos_sum = sum(pos_scores)\n",
    "        neg_sum = sum(neg_scores)\n",
    "        senti_avg = (pos_sum-neg_sum)/len(token)\n",
    "    else:\n",
    "        max_pos = 0\n",
    "        max_neg =0\n",
    "        pos_sum = 0\n",
    "        neg_sum =0\n",
    "        senti_avg =0\n",
    "\n",
    "    imbal = pos_sum - neg_sum\n",
    "    \n",
    "    positive_sum.append(pos_sum)\n",
    "    negative_sum.append(neg_sum)\n",
    "    averagesenti.append(senti_avg)\n",
    "    pos_neg_diff.append(imbal)\n",
    "    sentiments.append(str(pos_sum)+\"\\t\"+str(neg_sum)+\"\\t\"+str(senti_avg)+\"\\t\"+str(imbal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_to_file(sentiments,\"sentiments\",\"positive_sum\" + \"\\t\" + \"negative_sum\"+ \"\\t\" + \"averagesenti\"+ \"\\t\" + \"pos_neg_diff\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
