{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_vector = np.load(\"unigram_vector.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fo = open('text_stats.txt')\n",
    "text = fo.readlines()\n",
    "text = text[1:]\n",
    "char_count=[]\n",
    "word_count=[]\n",
    "avg_word_len=[]\n",
    "for row in text:\n",
    "    split = row.split('\\t')\n",
    "    char_count.append(split[0])\n",
    "    word_count.append(split[1])\n",
    "    avg_word_len.append(split[2])\n",
    "chars = np.array(char_count).astype(np.float)\n",
    "words = np.array(word_count).astype(np.float)\n",
    "avg_word_length = np.array(avg_word_len).astype(np.float)\n",
    "text_stats = np.column_stack((chars,words,avg_word_length)).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fo = open('punctuation.txt')\n",
    "text = fo.readlines()\n",
    "text = text[1:]\n",
    "punc = []\n",
    "ellipses =[]\n",
    "for row in text:\n",
    "    split = row.split('\\t')\n",
    "    punc.append(split[0])\n",
    "    ellipses.append(split[1])\n",
    "punc_count = np.array(punc).astype(np.float)\n",
    "ellipses_count = np.array(ellipses).astype(np.float)\n",
    "punctuations = np.column_stack((punc_count,ellipses_count)).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count_noun\tcount_verb\tcount_adverb\tcount_adjective\n",
    "count_noun = []\n",
    "count_verb = []\n",
    "count_adverb = []\n",
    "count_adjective = []\n",
    "\n",
    "fo = open('POS_tags.txt')\n",
    "text = fo.readlines()\n",
    "text=text[1:]\n",
    "for row in text:\n",
    "    split = row.split('\\t')\n",
    "    count_noun.append(split[0])\n",
    "    count_verb.append(split[1])\n",
    "    count_adverb.append(split[2])\n",
    "    count_adjective.append(split[3])\n",
    "\n",
    "array_noun=np.array(count_noun).astype(np.float)\n",
    "array_verb=np.array(count_verb).astype(np.float)\n",
    "array_adverb=np.array(count_adverb).astype(np.float)\n",
    "array_adjective=np.array(count_adjective).astype(np.float)\n",
    "pos_tags = np.column_stack((array_noun,array_verb,count_adverb,count_adjective)).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#emoticons\tlaugh_length\n",
    "emoticons = []\n",
    "laugh_length = []\n",
    "\n",
    "fo = open('emoticons_and_laughs.txt')\n",
    "text = fo.readlines()\n",
    "text=text[1:]\n",
    "for row in text:\n",
    "    split = row.split('\\t')\n",
    "    emoticons.append(split[0])\n",
    "    laugh_length.append(split[1])\n",
    "\n",
    "array_emoticons=np.array(emoticons).astype(np.float)\n",
    "array_laugh_len=np.array(laugh_length).astype(np.float)\n",
    "emojis_laughs = np.column_stack((emoticons,laugh_length)).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.column_stack((unigram_vector,punctuations,pos_tags,text_stats,emojis_laughs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "import re\n",
    "\n",
    "fo = open(\"labels.txt\")\n",
    "txt = fo.readlines()\n",
    "txt = txt[1:]\n",
    "labels = []\n",
    "for row in txt:\n",
    "    num = re.sub(\"\\n\",\"\",row)\n",
    "    labels.append(int(num))\n",
    "\n",
    "label_arr = np.array(labels).astype(np.float)\n",
    "K_FOLDS = 10 # 10-fold crossvalidation\n",
    "\n",
    "CLF2 = GaussianNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred2 = cross_val_predict(CLF2, data, labels, cv=K_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score,Accuracy,precision and recall New features included- GaussianNB 0.6025986761461143 0.5772039645279082 0.5668819188191881 0.6431187859759289\n",
      "confusion matrix [[ 984  939]\n",
      " [ 682 1229]]\n"
     ]
    }
   ],
   "source": [
    "scor2 = metrics.f1_score(labels,pred2 , pos_label = 1)\n",
    "acc2= metrics.accuracy_score(labels,pred2)\n",
    "prec2 = metrics.precision_score(labels,pred2)\n",
    "reca2 = metrics.recall_score(labels,pred2)\n",
    "a = metrics.confusion_matrix(labels,pred2)\n",
    "print(\"F1-score,Accuracy,precision and recall New features included- GaussianNB\",scor2,acc2,prec2,reca2)\n",
    "print(\"confusion matrix\",a)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
