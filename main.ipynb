{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "unigram_vector = np.load(\"unigram_vector.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = open('text_stats.txt')\n",
    "text = fo.readlines()\n",
    "text = text[1:]\n",
    "char_count=[]\n",
    "word_count=[]\n",
    "avg_word_len=[]\n",
    "for row in text:\n",
    "    split = row.split('\\t')\n",
    "    char_count.append(split[0])\n",
    "    word_count.append(split[1])\n",
    "    avg_word_len.append(split[2])\n",
    "chars = np.array(char_count).astype(np.float)\n",
    "words = np.array(word_count).astype(np.float)\n",
    "avg_word_length = np.array(avg_word_len).astype(np.float)\n",
    "text_stats = np.column_stack((chars,words,avg_word_length)).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = open('punctuation.txt')\n",
    "text = fo.readlines()\n",
    "text = text[1:]\n",
    "punc = []\n",
    "ellipses =[]\n",
    "for row in text:\n",
    "    split = row.split('\\t')\n",
    "    punc.append(split[0])\n",
    "    ellipses.append(split[1])\n",
    "punc_count = np.array(punc).astype(np.float)\n",
    "ellipses_count = np.array(ellipses).astype(np.float)\n",
    "punctuations = np.column_stack((punc_count,ellipses_count)).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#char_count\tword_count\tcount_noun\tcount_verb\tcount_adverb\tcount_adjective\n",
    "char_count = []\n",
    "word_count = []\n",
    "count_noun = []\n",
    "count_verb = []\n",
    "count_adverb = []\n",
    "count_adjective = []\n",
    "\n",
    "fo = open('POS_tags.txt')\n",
    "text = fo.readlines()\n",
    "text=text[1:]\n",
    "for row in text:\n",
    "    split = row.split('\\t')\n",
    "    count_noun.append(split[0])\n",
    "    count_verb.append(split[1])\n",
    "    count_adverb.append(split[2])\n",
    "    count_adjective.append(split[3])\n",
    "\n",
    "array_noun=np.array(count_noun).astype(np.float)\n",
    "array_verb=np.array(count_verb).astype(np.float)\n",
    "array_adverb=np.array(count_adverb).astype(np.float)\n",
    "array_adjective=np.array(count_adjective).astype(np.float)\n",
    "pos_tags = np.column_stack((array_noun,array_verb,count_adverb,count_adjective)).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.column_stack((unigram_vector,punctuations,pos_tags,text_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.           0.           0.         ... 100.          12.\n",
      "    7.33333333]\n",
      " [  0.           0.           0.         ... 126.          21.\n",
      "    4.9047619 ]\n",
      " [  0.15361031   0.           0.         ...  54.           9.\n",
      "    5.11111111]\n",
      " ...\n",
      " [  0.           0.           0.         ... 102.          19.\n",
      "    4.42105263]\n",
      " [  0.           0.           0.         ... 136.          17.\n",
      "    7.        ]\n",
      " [  0.13639069   0.           0.         ... 102.          20.\n",
      "    4.1       ]]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "import re\n",
    "\n",
    "fo = open(\"labels.txt\")\n",
    "txt = fo.readlines()\n",
    "txt = txt[1:]\n",
    "labels = []\n",
    "for row in txt:\n",
    "    num = re.sub(\"\\n\",\"\",row)\n",
    "    labels.append(int(num))\n",
    "\n",
    "label_arr = np.array(labels).astype(np.float)\n",
    "K_FOLDS = 10 # 10-fold crossvalidation\n",
    "\n",
    "CLF2 = GaussianNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = cross_val_predict(CLF2, data, labels, cv=K_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score,Accuracy,precision and recall New features included- GaussianNB 0.6168094466311645 0.5683359415753781 0.553156146179402 0.6970172684458399\n",
      "confusion matrix [[ 847 1076]\n",
      " [ 579 1332]]\n"
     ]
    }
   ],
   "source": [
    "scor2 = metrics.f1_score(labels,pred2 , pos_label = 1)\n",
    "acc2= metrics.accuracy_score(labels,pred2)\n",
    "prec2 = metrics.precision_score(labels,pred2)\n",
    "reca2 = metrics.recall_score(labels,pred2)\n",
    "a = metrics.confusion_matrix(labels,pred2)\n",
    "print(\"F1-score,Accuracy,precision and recall New features included- GaussianNB\",scor2,acc2,prec2,reca2)\n",
    "print(\"confusion matrix\",a)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
